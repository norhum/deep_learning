{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download the model and dataset from Hugging Face and fine-tune it using LoRA.\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"tiiuae/Falcon3-1B-Base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:32:25.099518Z","iopub.execute_input":"2025-02-10T15:32:25.099864Z","iopub.status.idle":"2025-02-10T15:34:09.598910Z","shell.execute_reply.started":"2025-02-10T15:32:25.099837Z","shell.execute_reply":"2025-02-10T15:34:09.598284Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/362k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf755173f9f4d9ab18ffbe7e3fd4931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36903ea80c834c4ebfd7648a1cd7da6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/826 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153355eb533249e781f0a865f95cd8e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcd6e0b38f674e7281a615703ba92dc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a33b15952e641f49e7514cc8f2bff70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f8f5a04a02349f2a600d32f312f6e2d"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch\nprint(torch.cuda.device_count())  # Check number of GPUs available\nprint(training_args.local_rank)  # Should be set for multi-GPU runs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig\n\n# Define LoRA configuration\nlora_config = LoraConfig(\n    r=8,  # Rank for LoRA\n    lora_alpha=32,  # Scaling factor\n    lora_dropout=0.1,  # Dropout rate for LoRA layers\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA to the base model\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:37:08.489145Z","iopub.execute_input":"2025-02-10T15:37:08.489830Z","iopub.status.idle":"2025-02-10T15:37:08.974045Z","shell.execute_reply.started":"2025-02-10T15:37:08.489802Z","shell.execute_reply":"2025-02-10T15:37:08.973322Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import DatasetDict\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"yahma/alpaca-cleaned\")\n\n# Assuming 'dataset' has a 'train' split\ntrain_dataset = dataset[\"train\"]\n\n# Split the 'train' dataset into train (80%) and eval (20%)\ntrain_split = train_dataset.train_test_split(test_size=0.2)\n\n# Now you have train_split['train'] and train_split['test']\ntrain_dataset = train_split['train']  # 80% for training\neval_dataset = train_split['test']   # 20% for evaluation\n\n# Function to merge instruction and input into a single string\ndef merge_instruction_input(example):\n    # Concatenate instruction and input, you can add a separator if needed\n    example['merged_input'] = example['instruction'] + \" \" + example['input']\n    return example\n\n# Apply the merge function to both train and eval datasets\ntrain_dataset = train_dataset.map(merge_instruction_input)\neval_dataset = eval_dataset.map(merge_instruction_input)\n\n# Function to tokenize the merged input and output\ndef tokenize_function(example):\n    # Tokenize the 'merged_input' and 'output'\n    input_encoding = tokenizer(example['merged_input'], padding=\"max_length\", truncation=True, max_length=256)\n    target_encoding = tokenizer(example['output'], padding=\"max_length\", truncation=True, max_length=256)\n\n    # Set input_ids and labels\n    input_encoding['labels'] = target_encoding['input_ids']  # Use output as labels\n    return input_encoding\n\n# Apply the tokenization to both train and eval datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\neval_dataset = eval_dataset.map(tokenize_function, batched=True)\n\n# List of columns to remove\ncolumns_to_remove = ['output', 'input', 'instruction', 'merged_input']\n\n# Remove the columns\ntrain_dataset = train_dataset.remove_columns(columns_to_remove)\neval_dataset = eval_dataset.remove_columns(columns_to_remove)\n\n# Print the modified train and eval datasets\nprint(\"Train Dataset:\")\nprint(train_dataset)\n\nprint(\"\\nEval Dataset:\")\nprint(eval_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:37:11.641879Z","iopub.execute_input":"2025-02-10T15:37:11.642211Z","iopub.status.idle":"2025-02-10T15:37:43.411911Z","shell.execute_reply.started":"2025-02-10T15:37:11.642185Z","shell.execute_reply":"2025-02-10T15:37:43.411003Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1e0d7d000246ccb842d5b1870b4316"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8b5881fd1f54240b6dd2c6801273f82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f8eacb7121c4e449ca539534369323a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/41408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ff93890f7a4731a74f8387fa2c6c1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10352 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"442a12237b9d4243b9bb1837d9f706fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/41408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6edda8e4f0542b184409126e5b2a645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10352 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4eb7077021841e19a7a42ac56d28269"}},"metadata":{}},{"name":"stdout","text":"Train Dataset:\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 41408\n})\n\nEval Dataset:\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 10352\n})\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Use wandb to log and save the model","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom transformers import Trainer, TrainingArguments\nfrom huggingface_hub import login\nlogin(\"hf_token\")\n\nwandb.login(key=\"wandb_token\")\n# Example fine-tuning parameters\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",              # Directory where model checkpoints and logs will be saved\n    per_device_train_batch_size=8,       # Batch size per GPU (if multiple GPUs, total batch size = batch_size * num_gpus)\n    per_device_eval_batch_size=8,        # Batch size per GPU for evaluation\n    evaluation_strategy=\"epoch\",         # Evaluate at the end of every epoch\n    num_train_epochs=3,                  # Train for 3 full passes through the dataset\n    logging_dir=\"./logs\",                # Directory for logs (useful for TensorBoard)\n    save_strategy=\"epoch\",               # Save checkpoints at the end of each epoch\n    save_total_limit=2,                  # Keep only the last 2 checkpoints, deleting older ones\n    report_to=\"wandb\",                   # Report training metrics to Weights & Biases\n    push_to_hub=True,                    # Push model checkpoints to Hugging Face Hub\n    fp16=True,                           # Enable mixed precision (use bf16=True for newer GPUs)\n    torch_compile=True,                  # Enable PyTorch 2.0 compilation\n    ddp_find_unused_parameters=False,    # Optimize DDP (if using multiple GPUs)\n    gradient_accumulation_steps=2,       # Simulates larger batch size without extra GPU memory\n    save_steps=500,                      # Save every 500 steps instead of just every epoch\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,  # Your custom dataset\n    eval_dataset=eval_dataset,  # Your validation dataset\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T15:41:13.667332Z","iopub.execute_input":"2025-02-10T15:41:13.667694Z","iopub.status.idle":"2025-02-11T02:19:32.299565Z","shell.execute_reply.started":"2025-02-10T15:41:13.667666Z","shell.execute_reply":"2025-02-11T02:19:32.298812Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250210_154114-2ll40dhn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/1257979-konkuk-university/huggingface/runs/2ll40dhn' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/1257979-konkuk-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/1257979-konkuk-university/huggingface' target=\"_blank\">https://wandb.ai/1257979-konkuk-university/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/1257979-konkuk-university/huggingface/runs/2ll40dhn' target=\"_blank\">https://wandb.ai/1257979-konkuk-university/huggingface/runs/2ll40dhn</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15528' max='15528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15528/15528 10:38:07, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>7.483900</td>\n      <td>7.477202</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>7.391100</td>\n      <td>7.422033</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>7.371800</td>\n      <td>7.405322</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15528, training_loss=7.509777732389235, metrics={'train_runtime': 38296.9512, 'train_samples_per_second': 3.244, 'train_steps_per_second': 0.405, 'total_flos': 2.6751395593558426e+17, 'train_loss': 7.509777732389235, 'epoch': 3.0})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import wandb\n\nartifact = wandb.Artifact(\"model_checkpoint\", type=\"model\")\nartifact.add_dir(\"./results/\")  # Upload all checkpoint files\nwandb.log_artifact(artifact)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T02:19:43.869953Z","iopub.execute_input":"2025-02-11T02:19:43.870248Z","iopub.status.idle":"2025-02-11T02:19:44.085862Z","shell.execute_reply.started":"2025-02-11T02:19:43.870206Z","shell.execute_reply":"2025-02-11T02:19:44.085161Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./results)... Done. 0.0s\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<Artifact model_checkpoint>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Import the model artifact from wandb and compare its performance to the base model.","metadata":{}},{"cell_type":"code","source":"from transformers import  AutoModelForCausalLM\n\nimport wandb\nwandb.init()\nartifact = wandb.use_artifact(\"1257979-konkuk-university/huggingface/model_checkpoint:latest\", type=\"model\")\nartifact_dir = artifact.download()\n\n# Load the model\nfrom transformers import AutoModel\nmodel = AutoModelForCausalLM.from_pretrained(artifact_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T04:02:12.864089Z","iopub.execute_input":"2025-02-11T04:02:12.864385Z","iopub.status.idle":"2025-02-11T04:02:17.438664Z","shell.execute_reply.started":"2025-02-11T04:02:12.864365Z","shell.execute_reply":"2025-02-11T04:02:17.437687Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   19 of 19 files downloaded.  \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoTokenizer, GenerationConfig\n\nmodel_name = \"tiiuae/Falcon3-1B-Base\"\nmodel.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntext = \"What is the capital of South Korea?\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(**inputs.to(model.device), max_new_tokens=10)\n\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T04:27:32.479069Z","iopub.execute_input":"2025-02-11T04:27:32.479354Z","iopub.status.idle":"2025-02-11T04:27:36.323900Z","shell.execute_reply.started":"2025-02-11T04:27:32.479331Z","shell.execute_reply":"2025-02-11T04:27:36.323099Z"}},"outputs":[{"name":"stdout","text":"What is the capital of South Korea? Seoul\n\n.\n\n.\n\n\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\nmodel_name = \"tiiuae/Falcon3-1B-Base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel2 = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cuda\")\nmodel2.generation_config = GenerationConfig.from_pretrained(model_name)\nmodel2.generation_config.pad_token_id = model2.generation_config.eos_token_id\n\ntext = \"What is the capital of South Korea?\"\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model2.generate(**inputs.to(model2.device), max_new_tokens=50)\n\nresult = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T04:27:28.526195Z","iopub.execute_input":"2025-02-11T04:27:28.526435Z","iopub.status.idle":"2025-02-11T04:27:32.477819Z","shell.execute_reply.started":"2025-02-11T04:27:28.526414Z","shell.execute_reply":"2025-02-11T04:27:32.477051Z"}},"outputs":[{"name":"stdout","text":"What is the capital of South Korea?\n\n\nWhat is the capital of South Korea?\n\n\nWhat is the capital of South Korea?\n\n\nWhat is the capital of South Korea?\n\n\nWhat is the capital of South Korea?\n\n\nWhat is the\n","output_type":"stream"}],"execution_count":21}]}