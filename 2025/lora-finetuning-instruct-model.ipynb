{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the model and dataset from Hugging Face and fine-tune it using LoRA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:32:25.099864Z",
     "iopub.status.busy": "2025-02-10T15:32:25.099518Z",
     "iopub.status.idle": "2025-02-10T15:34:09.598910Z",
     "shell.execute_reply": "2025-02-10T15:34:09.598284Z",
     "shell.execute_reply.started": "2025-02-10T15:32:25.099837Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf755173f9f4d9ab18ffbe7e3fd4931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/362k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36903ea80c834c4ebfd7648a1cd7da6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153355eb533249e781f0a865f95cd8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd6e0b38f674e7281a615703ba92dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a33b15952e641f49e7514cc8f2bff70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8f5a04a02349f2a600d32f312f6e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model_name = \"tiiuae/Falcon3-1B-Base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:37:08.489830Z",
     "iopub.status.busy": "2025-02-10T15:37:08.489145Z",
     "iopub.status.idle": "2025-02-10T15:37:08.974045Z",
     "shell.execute_reply": "2025-02-10T15:37:08.973322Z",
     "shell.execute_reply.started": "2025-02-10T15:37:08.489802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank for LoRA\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA layers\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:37:11.642211Z",
     "iopub.status.busy": "2025-02-10T15:37:11.641879Z",
     "iopub.status.idle": "2025-02-10T15:37:43.411911Z",
     "shell.execute_reply": "2025-02-10T15:37:43.411003Z",
     "shell.execute_reply.started": "2025-02-10T15:37:11.642185Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1e0d7d000246ccb842d5b1870b4316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b5881fd1f54240b6dd2c6801273f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8eacb7121c4e449ca539534369323a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ff93890f7a4731a74f8387fa2c6c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442a12237b9d4243b9bb1837d9f706fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6edda8e4f0542b184409126e5b2a645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4eb7077021841e19a7a42ac56d28269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 41408\n",
      "})\n",
      "\n",
      "Eval Dataset:\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10352\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "\n",
    "# Assuming 'dataset' has a 'train' split\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "# Split the 'train' dataset into train (80%) and eval (20%)\n",
    "train_split = train_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Now you have train_split['train'] and train_split['test']\n",
    "train_dataset = train_split['train']  # 80% for training\n",
    "eval_dataset = train_split['test']   # 20% for evaluation\n",
    "\n",
    "# Function to merge instruction and input into a single string\n",
    "def merge_instruction_input(example):\n",
    "    # Concatenate instruction and input, you can add a separator if needed\n",
    "    example['merged_input'] = example['instruction'] + \" \" + example['input']\n",
    "    return example\n",
    "\n",
    "# Apply the merge function to both train and eval datasets\n",
    "train_dataset = train_dataset.map(merge_instruction_input)\n",
    "eval_dataset = eval_dataset.map(merge_instruction_input)\n",
    "\n",
    "# Function to tokenize the merged input and output\n",
    "def tokenize_function(example):\n",
    "    # Tokenize the 'merged_input' and 'output'\n",
    "    input_encoding = tokenizer(example['merged_input'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    target_encoding = tokenizer(example['output'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    # Set input_ids and labels\n",
    "    input_encoding['labels'] = target_encoding['input_ids']  # Use output as labels\n",
    "    return input_encoding\n",
    "\n",
    "# Apply the tokenization to both train and eval datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# List of columns to remove\n",
    "columns_to_remove = ['output', 'input', 'instruction', 'merged_input']\n",
    "\n",
    "# Remove the columns\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "eval_dataset = eval_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Print the modified train and eval datasets\n",
    "print(\"Train Dataset:\")\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"\\nEval Dataset:\")\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use wandb to log and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T15:41:13.667694Z",
     "iopub.status.busy": "2025-02-10T15:41:13.667332Z",
     "iopub.status.idle": "2025-02-11T02:19:32.299565Z",
     "shell.execute_reply": "2025-02-11T02:19:32.298812Z",
     "shell.execute_reply.started": "2025-02-10T15:41:13.667666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250210_154114-2ll40dhn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/1257979-konkuk-university/huggingface/runs/2ll40dhn' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/1257979-konkuk-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/1257979-konkuk-university/huggingface' target=\"_blank\">https://wandb.ai/1257979-konkuk-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/1257979-konkuk-university/huggingface/runs/2ll40dhn' target=\"_blank\">https://wandb.ai/1257979-konkuk-university/huggingface/runs/2ll40dhn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15528' max='15528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15528/15528 10:38:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.483900</td>\n",
       "      <td>7.477202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.391100</td>\n",
       "      <td>7.422033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.371800</td>\n",
       "      <td>7.405322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15528, training_loss=7.509777732389235, metrics={'train_runtime': 38296.9512, 'train_samples_per_second': 3.244, 'train_steps_per_second': 0.405, 'total_flos': 2.6751395593558426e+17, 'train_loss': 7.509777732389235, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from huggingface_hub import login\n",
    "login(\"hf_token\")\n",
    "\n",
    "wandb.login(key=\"wandb_token\")\n",
    "# Example fine-tuning parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Directory where model checkpoints and logs will be saved\n",
    "    per_device_train_batch_size=8,       # Batch size per GPU (if multiple GPUs, total batch size = batch_size * num_gpus)\n",
    "    per_device_eval_batch_size=8,        # Batch size per GPU for evaluation\n",
    "    evaluation_strategy=\"epoch\",         # Evaluate at the end of every epoch\n",
    "    num_train_epochs=3,                  # Train for 3 full passes through the dataset\n",
    "    logging_dir=\"./logs\",                # Directory for logs (useful for TensorBoard)\n",
    "    save_strategy=\"epoch\",               # Save checkpoints at the end of each epoch\n",
    "    save_total_limit=2,                  # Keep only the last 2 checkpoints, deleting older ones\n",
    "    report_to=\"wandb\",                   # Report training metrics to Weights & Biases\n",
    "    push_to_hub=True,                    # Push model checkpoints to Hugging Face Hub\n",
    "    fp16=True,                           # Enable mixed precision (use bf16=True for newer GPUs)\n",
    "    torch_compile=True,                  # Enable PyTorch 2.0 compilation\n",
    "    ddp_find_unused_parameters=False,    # Optimize DDP (if using multiple GPUs)\n",
    "    gradient_accumulation_steps=2,       # Simulates larger batch size without extra GPU memory\n",
    "    save_steps=500,                      # Save every 500 steps instead of just every epoch\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  # Your custom dataset\n",
    "    eval_dataset=eval_dataset,  # Your validation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T02:19:43.870248Z",
     "iopub.status.busy": "2025-02-11T02:19:43.869953Z",
     "iopub.status.idle": "2025-02-11T02:19:44.085862Z",
     "shell.execute_reply": "2025-02-11T02:19:44.085161Z",
     "shell.execute_reply.started": "2025-02-11T02:19:43.870206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./results)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Artifact model_checkpoint>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "artifact = wandb.Artifact(\"model_checkpoint\", type=\"model\")\n",
    "artifact.add_dir(\"./results/\")  # Upload all checkpoint files\n",
    "wandb.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the model artifact from wandb and compare its performance to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:02:12.864385Z",
     "iopub.status.busy": "2025-02-11T04:02:12.864089Z",
     "iopub.status.idle": "2025-02-11T04:02:17.438664Z",
     "shell.execute_reply": "2025-02-11T04:02:17.437687Z",
     "shell.execute_reply.started": "2025-02-11T04:02:12.864365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   19 of 19 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "from transformers import  AutoModelForCausalLM\n",
    "\n",
    "import wandb\n",
    "wandb.init()\n",
    "artifact = wandb.use_artifact(\"1257979-konkuk-university/huggingface/model_checkpoint:latest\", type=\"model\")\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load the model\n",
    "from transformers import AutoModel\n",
    "model = AutoModelForCausalLM.from_pretrained(artifact_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:27:32.479354Z",
     "iopub.status.busy": "2025-02-11T04:27:32.479069Z",
     "iopub.status.idle": "2025-02-11T04:27:36.323900Z",
     "shell.execute_reply": "2025-02-11T04:27:36.323099Z",
     "shell.execute_reply.started": "2025-02-11T04:27:32.479331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of South Korea? Seoul\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "\n",
    "model_name = \"tiiuae/Falcon3-1B-Base\"\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text = \"What is the capital of South Korea?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs.to(model.device), max_new_tokens=10)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T04:27:28.526435Z",
     "iopub.status.busy": "2025-02-11T04:27:28.526195Z",
     "iopub.status.idle": "2025-02-11T04:27:32.477819Z",
     "shell.execute_reply": "2025-02-11T04:27:32.477051Z",
     "shell.execute_reply.started": "2025-02-11T04:27:28.526414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of South Korea?\n",
      "\n",
      "\n",
      "What is the capital of South Korea?\n",
      "\n",
      "\n",
      "What is the capital of South Korea?\n",
      "\n",
      "\n",
      "What is the capital of South Korea?\n",
      "\n",
      "\n",
      "What is the capital of South Korea?\n",
      "\n",
      "\n",
      "What is the\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model_name = \"tiiuae/Falcon3-1B-Base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "model2.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "model2.generation_config.pad_token_id = model2.generation_config.eos_token_id\n",
    "\n",
    "text = \"What is the capital of South Korea?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model2.generate(**inputs.to(model2.device), max_new_tokens=50)\n",
    "\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I was able to 10x train the model faster using mixed precision and ddp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm  # For progress bars\n",
    "import time\n",
    "\n",
    "# --- Model and Tokenizer Setup ---\n",
    "model_name = \"tiiuae/Falcon3-1B-Base\"  # Or any other Falcon model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using {num_gpus} {device} GPUs\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high') #sets to tf32 during matmul operations\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model = model.to(device)  # Move to device *before* DataParallel\n",
    "# model = torch.compile(model)\n",
    "\n",
    "# --- LoRA Configuration ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- DataParallel (AFTER moving to device and applying LoRA) ---\n",
    "if num_gpus > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# --- Dataset Loading and Preprocessing ---\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_split = train_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_split['train']\n",
    "eval_dataset = train_split['test']\n",
    "\n",
    "def merge_instruction_input(example):\n",
    "    example['merged_input'] = example['instruction'] + \" \" + example['input']\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(merge_instruction_input)\n",
    "eval_dataset = eval_dataset.map(merge_instruction_input)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    input_encoding = tokenizer(example['merged_input'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    target_encoding = tokenizer(example['output'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    input_encoding['labels'] = target_encoding['input_ids']\n",
    "    return input_encoding\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "columns_to_remove = ['output', 'input', 'instruction', 'merged_input']\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "eval_dataset = eval_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "num_epochs = 2\n",
    "learning_rate = 2e-4\n",
    "batch_size_per_gpu = 8  # Batch size *per GPU*\n",
    "batch_size = batch_size_per_gpu * num_gpus  # *Effective* batch size\n",
    "gradient_accumulation_steps = 4  # Adjust if needed\n",
    "warmup_steps = 50\n",
    "output_dir = \"falcon-lora-alpaca\"\n",
    "eval_steps = 200\n",
    "save_steps = 200\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size) # eval batch size can often be larger\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * num_epochs // gradient_accumulation_steps\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    t0 = time.time()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss  # This is now a tensor, e.g., shape (num_gpus,)\n",
    "\n",
    "        # --- KEY CHANGE: Reduce the loss to a scalar ---\n",
    "        loss = loss.mean()  # Take the mean across the GPU losses\n",
    "\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.synchronize()\n",
    "            t1 = time.time()\n",
    "            dt = (t1 - t0)\n",
    "            t0 = t1\n",
    "            tokens_processed = train_dataloader.batch_size * 256 * gradient_accumulation_steps #256 is the max_length for the tokenizer\n",
    "            tokens_per_sec = tokens_processed / dt\n",
    "            print(f\"dt: {dt:.2f}sec, tok/sec: {tokens_per_sec:.2f}\")\n",
    "\n",
    "        if (step + 1) % eval_steps == 0:\n",
    "            model.eval()\n",
    "            eval_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for eval_batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "                    eval_batch = {k: v.to(device) for k, v in eval_batch.items()}\n",
    "                    eval_outputs = model(**eval_batch)\n",
    "                    eval_loss += eval_outputs.loss.detach().float()\n",
    "\n",
    "            avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "            print(f\"Step {step+1}: Eval Loss: {avg_eval_loss:.4f}\")\n",
    "            model.train()  # Switch back to train mode\n",
    "\n",
    "        if (step + 1) % save_steps == 0:\n",
    "            checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{step + 1}\")\n",
    "            # Save the *underlying* model (important for DataParallel)\n",
    "            if num_gpus > 1:\n",
    "                model.module.save_pretrained(checkpoint_dir)\n",
    "            else:\n",
    "                model.save_pretrained(checkpoint_dir)\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader) * gradient_accumulation_steps\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# Save final model (again, handle DataParallel)\n",
    "if num_gpus > 1:\n",
    "    model.module.save_pretrained(output_dir)\n",
    "else:\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
